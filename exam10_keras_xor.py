# -*- coding: utf-8 -*-
"""exam10_keras_xor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZggPlNmYUv7o2MpCX4oZU48mQnY8xIy0

## 인공지능의 역사
https://itwiki.kr/w/%ec%9d%b8%ea%b3%b5%ec%a7%80%eb%8a%a5
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

training_data = np.array([[0,0], [0,1], [1,0], [1,1]], 'float32')
target_data = np.array([[0], [1], [1], [0]], 'float32')

model = Sequential() # layer 들이 Sequential하게
model.add(Dense(32, input_dim=2, activation='relu')) #input_dim = 2 최초의 것이 들어오는 것은 layer로 치지 않는다 32개의 Dense 처음, 마지막 빼고는 hidden Layer
model.add(Dense(1, activation='sigmoid')) # 32개의 Dense가 1개로 모이고 마지막은 출력 layer
model.compile(loss = 'mse', optimizer= 'adam', metrics=['binary_accuracy']) # optimizer = 경사하강 알고리즘을 어떤 것을 쓰는지, adam은 learning_rate를 알아서 조절해 준다.
print(model.summary())

fit_hist = model.fit(training_data, target_data, epochs = 500, verbose = 1) #verbose : 뒤에 나오는 0s 9ms/step - loss: 0.0593 - binary_accuracy: 1.0000의 정보를 얼마나 보여주는지 작을 수록

plt.plot(fit_hist.history['loss'])
plt.show()

plt.plot(fit_hist.history['binary_accuracy'])
plt.show()

inp = list(map(int, input('논리값을 입력하시오').split()))
qwe = np.array(inp) #입력값을 array type으로
print('입력 값:', qwe)
qwe = qwe.reshape(-1, 2) # 2행으로 만들어 주는, 알아서해 마법의주문 = -1
print('reshape :', qwe)
print('결과 값 :', model.predict(qwe)[0][0].round())

"""#경사하강 알고리즘"""

