# -*- coding: utf-8 -*-
"""exam24_iris02_knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F5XjX7RFbJt43LvuvNyNhxCCsaIrt_AX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

plt.rcParams['figure.figsize'] = [7, 7]
sns.set(style='darkgrid')
plt.rcParams['scatter.edgecolors'] = 'black'
pd.set_option('display.max_columns', None)
pd.set_option('display.max_row', None)
pd.set_option('display.unicode.east_asian_width', True)

iris_dataset = load_iris()
iris = pd.DataFrame(iris_dataset.data,
        columns=iris_dataset.feature_names)
labels = iris_dataset.target_names
iris.info()
print(iris.head())

labels

label = iris_dataset.target
print(label)

scaler = StandardScaler()
iris = scaler.fit_transform(iris) # 평균 0 표준 편차가 1인 분포를 따르는 상태
Features = pd.DataFrame(iris, columns= ['SL', 'SW', 'PL', 'PW'])
print(Features.shape)

X_train, X_test, Y_train, Y_test = train_test_split(
    Features, label, test_size = 0.2) 
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

accuracy_rate = []
error_rate = []
for i in range(1,50):
  knn = KNeighborsClassifier(n_neighbors=i)
  score = cross_val_score(knn, Features, label, cv=17) # knn모델로 분류, Features를 주고 label도 cv(cross values),  k 교차 검증:ex.20분류로 나누고 19개로 학습, 나머지 하나로 test test_set는 돌아가면서-> 각각의 정확도를 본다 cv=20->20개의 등분
  accuracy_rate.append(score.mean())
  error_rate.append(1- score.mean())

plt.figure(figsize = (10, 6))
plt.plot(range(1,50), accuracy_rate, color='b',
         linestyle = 'dashed', marker = 'o', markerfacecolor = 'r', markersize = 10)
plt.title('Accuracy Rate vs K Value')
plt.xlabel('K')
plt.ylabel('Accuracy Rate')
plt.show()

plt.figure(figsize = (10, 6))
plt.plot(range(1,50), error_rate, color='r',
         linestyle = 'dashed', marker = '^', markerfacecolor = 'b', markersize = 10)
plt.title('Error_rate vs K Value')
plt.xlabel('K')
plt.ylabel('Error_rate')
plt.show()

model_predict_error_rate = []
for i in range(1, 50):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train, Y_train)
  pred_i = knn.predict(X_test)
  model_predict_error_rate.append(np.mean(pred_i != Y_test)) #pred_i != Y_test 비교연산자 True, False, 틀렸을 경우 1, 맞았을 경우 0-> 평균으로 나눈다-> error율

plt.figure(figsize = (10, 6))
plt.plot(range(1, 50), model_predict_error_rate, 
         linestyle = 'dashed', marker = 'o',
         color = 'b', markersize = 10)
plt.title('Model predict error rate vs K Value')
plt.xlabel('K')
plt.ylabel('Model predict error rate')
plt.show()
# train, test 를 나눌 때 random 이기 때문에 나눌 때 마다 다르다-> 샘플링을 어떻게 하냐에 따라 모델의 정확도는 다르다, 어떤 순서로 data를 넣는냐에 따라 결정경계가 만들어 지는 방식이 다르다

for i in range(1, 1000):
  X_train, X_test, Y_train, Y_test = train_test_split(
      Features, label, test_size = 0.2, random_state = i) # random_state-> random 하게 나눌 때 어떤 방식으로 random 하게 나누었는지 기억한다.
  IrisKNN = KNeighborsClassifier(n_neighbors = 6)
  IrisKNN.fit(X_train, Y_train)
  train_score = IrisKNN.score(X_train, Y_train)
  test_score = IrisKNN.score(X_test, Y_test)
  if test_score > train_score:
    print('Test: {} Train: {} RandomState: {}'.format(
        test_score, train_score, i))

X_train, X_test, Y_train, Y_test = train_test_split(
    Features, label, test_size = 0.2, random_state = 960) # random_state-> random 하게 나눌 때 어떤 방식으로 random 하게 나누었는지 기억한다.
IrisKNN = KNeighborsClassifier(n_neighbors = 6)
IrisKNN.fit(X_train, Y_train)
train_score = IrisKNN.score(X_train, Y_train)
print(train_score)
test_score = IrisKNN.score(X_test, Y_test)
print(test_score)

from sklearn.metrics import classification_report, confusion_matrix
pd.DataFrame(confusion_matrix(Y_test, IrisKNN.predict(X_test)),
             columns = ['P_setosa', 'P_versicolor', 'P_virginica'],
             index=['A_setosa', 'A_versicolor', 'A_virginica'])

print(classification_report(Y_test, IrisKNN.predict(X_test)))

from sklearn.model_selection import StratifiedShuffleSplit # 그룹별로 같은 개수를 랜덤하게 나누는 것
cv = StratifiedShuffleSplit(n_splits = 10, test_size =0.2)

accuracies = cross_val_score(IrisKNN, Features, label, cv = cv, scoring='accuracy')
print('Cross_Validation accuracy scores : {}'.format(accuracies))
print('Mean Cross-Validation accuracy score: {}'.format(round(accuracies.mean(),3)))

from sklearn.model_selection import GridSearchCV # 원래는 for문을 돌려야 한다. 하지만 GridSearchCV를 쓰면 알아서 최적값을 찾아준다

k_range = range(1, 50)
weights_options=['uniform', 'distance']
param = {'n_neighbors':k_range, 'weights':weights_options} # 옵션 중에 'uniform', 'distance'가 있는데 알아서 2개 돌려서 for 문을 돌려준다
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state = 1)
IrisKNN = GridSearchCV(KNeighborsClassifier(), 
            param, cv=cv, verbose=False, n_jobs=-1)
IrisKNN.fit(X_train, Y_train)

print(IrisKNN.best_score_)
print(IrisKNN.best_params_)
print(IrisKNN.best_estimator_)

IrisKNN = IrisKNN.best_estimator_
print(IrisKNN.score(X_test, Y_test))

pd.DataFrame(confusion_matrix(Y_test, IrisKNN.predict(X_test)),
             columns = ['P_setosa', 'P_versicolor', 'P_virginica'],
             index=['A_setosa', 'A_versicolor', 'A_virginica'])

