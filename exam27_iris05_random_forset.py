# -*- coding: utf-8 -*-
"""exam27_iris05_Random_Forset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F99zNNxGdZJj7qmx8Tw1sd93xc6Pf1ps
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.model_selection import *
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import tree

plt.rcParams['figure.figsize'] = [7, 7]
sns.set(style='darkgrid')
plt.rcParams['scatter.edgecolors'] = 'black'
pd.set_option('display.max_columns', None)
pd.set_option('display.max_row', None)
pd.set_option('display.unicode.east_asian_width', True)

iris_dataset = load_iris()
iris = pd.DataFrame(iris_dataset.data,
        columns=iris_dataset.feature_names)
labels = iris_dataset.target_names
iris.info()
print(iris.head())

label = iris_dataset.target
print(label)

scaler = StandardScaler()
iris = scaler.fit_transform(iris)
Features = pd.DataFrame(iris, columns=['SL', 'SW', 'PL', 'PW'])
print(Features.shape)

X_train, X_test, Y_train, Y_test = train_test_split(
    Features, label, test_size=0.2)
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

from scipy.sparse.construct import random
for i in range(1, 1000):
    X_train, X_test, Y_train, Y_test = train_test_split(
    Features, label, test_size=0.2, random_state=i)
    iris_Tree = DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.008,
                                       random_state = 2)
    iris_Forest = BaggingClassifier(iris_Tree, n_estimators= 200, #n_estimators-> 결정트리의 갯수를 지정
                                    max_samples = 0.8, random_state=868) # max_samples-> 최대 80%만 사용     
    iris_Forest.fit(X_train, Y_train)
    train_score = iris_Forest.score(X_train, Y_train)
    test_score = iris_Forest.score(X_test, Y_test)
    if test_score >= train_score:
      print('test: {} train : {} random_state : {}'.format(test_score, train_score, i))

X_train, X_test, Y_train, Y_test = train_test_split(
  Features, label, test_size=0.2, random_state=200)
iris_Tree = DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.008,
                                    random_state = 2)
iris_Forest = BaggingClassifier(iris_Tree, n_estimators= 200, #n_estimators-> 결정트리의 갯수를 지정
                                max_samples = 0.8, random_state=868) # max_samples-> 최대 80%만 사용     
iris_Forest.fit(X_train, Y_train)
train_score = iris_Forest.score(X_train, Y_train)
test_score = iris_Forest.score(X_test, Y_test)
if test_score >= train_score:
  print('test: {} train : {}'.format(test_score, train_score))

print(len(iris_Forest.estimators_))

print(iris_Forest.estimators_)

plt.figure(figsize=(15,8))
tree.plot_tree(iris_Forest.estimators_[3],
               filled=True, rounded=True,
               class_names = ['setosa', 'versicolor', 'viginica'],
               feature_names=Features.columns)

for i in range(len(iris_Forest.estimators_)):
  print(tree.export_text(iris_Forest.estimators_[i]))

pd.DataFrame(confusion_matrix(Y_test, iris_Forest.predict(X_test)),
        columns=['P_setosa', 'P_versicolor', 'P_virginica'],
        index=['A_setosa', 'A_versicolor', 'A_virginica'])

print(classification_report(Y_test, iris_Forest.predict(X_test)))