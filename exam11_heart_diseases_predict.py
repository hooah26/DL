# -*- coding: utf-8 -*-
"""exam11_heart_diseases_predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e1l1aHzZTwEUDIdD7aSbhL8Ak5djIg2j
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import matplotlib.pyplot as plt
import pandas as pd

column_name = ['age', 'sex', 'cp', 'treshbps', 'chol', 
               'fbs',
               'restecg', 'thalach', 'exang', 'oldpeak', 
               'slope',
               'ca', 'thal', 'HeartDisease']
raw_data = pd.read_excel('./datasets/heart-disease.xlsx',
                         header=None, names=column_name)
print(raw_data.head(20))

print(raw_data.describe().T)

raw_data.info()

clean_data = raw_data.replace('?', np.nan)
clean_data = clean_data.dropna()
print(clean_data.info())

keep = column_name.pop()
print(keep)
print(column_name)

training_data = clean_data[column_name]
target = clean_data[[keep]]
print(training_data.head())
print(target.head())

print(target[keep].sum())

print(target[keep].mean())

# 어떤 col이 중요한지는 아직 모르기 때문에 col의 스케일을 맞출 필요가 있다., but 명목측도는 another encoding을 해야한다
# cp는 가슴통증의 정도는 1 ~ 4로 나눈것이다. 그렇기 때문에 1개의 col로 하는 것이 아니라 각각의 통증 정도에 별개의 col을 만든다. -> 3 col의 경우 가슴통증이 3이면 1, 이외에는 0 즉 명목척도는 이런 전처리

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(training_data) # 표준정규분표로 만드는 함수
print(type(scaled_data))
scaled_data = pd.DataFrame(scaled_data, columns = column_name)
print(scaled_data.head())
# 평균이 0 표준편차가 1인 정규분호=> 표준정규분표

print(scaled_data.describe().T) #모든 data 가 평균이 0 표준편차가 1

scaled_data.boxplot(column = column_name, showmeans= True)# showmeans 도 나오게
# plt.show()

# 과대적합 = 답만 외운 것, data가 조금만 달라져도 답을 주기 힘든 것
# 과소적합 = 학습이 덜 된 상태

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(scaled_data, target, test_size = 0.30) # data 중 30%를 검증용으로 따로 보관한다. 랜덤하게
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
print('Y_train :', Y_train.shape)
print('Y_test :', Y_test.shape)

model = Sequential()
model.add(Dense(512 ,input_dim = 13, activation = 'relu')) # Param = 13*512+512 
model.add(Dropout(0.25)) # weight 중에 25%는 backward를 하지 않는다->weight, bias를 수정하지 않는다 , 매 epoch 마다 새롭게 25%를 뽑는다. 조절을 해 가며 찾아야 한다(경험적으로).
model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.2)) 
model.add(Dense(128, activation = 'relu'))
model.add(Dropout(0.1)) 
model.add(Dense(1, activation = 'sigmoid'))
model.summary()

model.compile(loss='mse', optimizer = 'adam', metrics= ['binary_accuracy']) # type1 error와 type2 error는 상보적관계, 데이터의 특성에 따라 민감도 특이도 정확도 등을 다르게 봐야 한다
fit_hist = model.fit(X_train, Y_train, batch_size = 50, epochs = 10, # batch_size => data를 50개씩 잘라서 학습, 50문제 이후 mse로 확인 후 재학습 
                     validation_split=0.2, verbose = 1) #validation_split = 첫 epochs에서는 80%를 뽑아서 학습, 그 후 나머지 20%를 가지고 검증을 한다

# binary_accuracy => 분류를 할 때 분류가 되었는지 확인 할 경우 , loss => 값을 예측할 때 얼마나 해당하는 값에 가까이 갔는지 확일할 때 
plt.plot(fit_hist.history['binary_accuracy']) # 80%가지고 구한 정확도
plt.plot(fit_hist.history['val_binary_accuracy']) # 
plt.show()

plt.plot(fit_hist.history['binary_accuracy']) # 80%가지고 구한 정확도
plt.plot(fit_hist.history['val_binary_accuracy']) # 
plt.show()

score = model.evaluate(X_test, Y_test, verbose= 0) # evaluate :학습을 하지 않고 loss, binary_accuracy  반환 , x 입력 y 타겟
print('Keras DNN model loss :',score[0])
print('Keras DNN model accuracy :',score[1]) 
# 무작정 많이 학습을 많이 하는 것이 좋은 것이 아니다. 학습을 많이 하게되면 과적합을 할 수 있다.

